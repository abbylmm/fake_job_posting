---
title: "WQD7004 Group Project"
output: github_document
---

![picture](https://dynamichealthstaff.com/blog/wp-content/uploads/2019/10/job-scam.jpg)

# Title : Fake Job Posting Analysis

### Group 4 members:

-   JingYu Shen (S2113037)
-   JiPing Zhang (S2042894)
-   Lee Mun Mun (S2112842)
-   Nayli Hatim (S2149344)
-   Jenifer Mayang Jues (S2016572)

# Introduction

Unlike the past when job seekers used newspapers to seek job opportunities, job seekers nowadays use employment website such as JobStreet, Linkedin, Indeed and countless due to advancement in modern technology and social communication. The authenticity of job postings has become critical with a constant increase in the number of job scams. According to Habiba et all (2021), job advertisements which are fake and steal personal & professional information of job seekers instead of giving right jobs to them is known as job scam. Job scams often involve fake online job ads in social platforms and untrusted job portals offering high paying jobs. Victims may also receive unsolicited messages from social media such as Whatsapp, Facebook, WeChat that offers jobs that do not exist. For example, job scammers will ask victims to disclose personal and/or banking details or transfer upfront fees to secure a interview or more information about the fraud jobs. Due to the growing concerns about job scams, our aim is to raise awareness of job seekers in the job application process and give a early warning sign to job seekers with Machine Learning (ML) and Natural Language Processing (NLP) approaches.

# Objectives

-   To identify the key features of fraudulent job postings.
-   To build a model to classify real or fake job postings.

# Initial Questions

-   What are the key features/characteristics of fraudulent job postings?
-   Which classification model is the best to determine whether the job is real or not?

# Data Cleaning and Pre-processing

The dataset used in this project was published by the Employment Scam Aegean Dataset (EMSCAD) and was retrieved from [Kaggle](https://www.kaggle.com/datasets/shivamb/real-or-fake-fake-jobposting-prediction) and this data contains 17,880 observations out of which about 866 are fake, and 18 features. The data consists of a combination of numeric and text features. A brief definition of the variables is given below:

| Variable            | Description                                                |
|---------------------|---------------------------------------------------|
| job_id              | ID of each job posting                                     |
| title               | Description of position or job                             |
| location            | Where the job is located                                   |
| department          | Department of the job offered                              |
| salary_range        | Expected salary range                                      |
| company_profile     | Company information                                        |
| description         | Description about the position offered                     |
| requirements        | Pre-requisites to qualify for the job                      |
| benefits            | Benefits provided by the job                               |
| telecommuting       | Is work from home or remote work allowed                   |
| has_company_logo    | Does the post have a company logo                          |
| has_questions       | Does the post have any questions                           |
| employment_type     | Full-time, part-time, contract, temporary and others       |
| required_experience | Experience level, e.g. Entry level, Executive, Director... |
| required_education  | Education level, e.g. High School, Bachelor, Master...     |
| industry            | Relevant industry                                          |
| function            | Job's functionality                                        |
| fraudulent          | Target variable (0: Real, 1: Fake)                         |

## Import libraries

```{r setup, include=FALSE}
library(dplyr)
library(naniar)
library(skimr)
library(stringr)
library(visdat)
library(DataExplorer)
library(ggplot2)
library(plotly)
library(RColorBrewer)
library(keras)
library(tidyr)
library(reshape2)
library(grid)
library(tidyverse)
library(shadowtext)
library(wordcloud)
library(wordcloud2)
library(tm)
library(webshot)
library(htmlwidgets)
library(corrplot)
library(ROCR)
library(data.table)
library(randomForest)
library(kknn)
library(caret)
```

## Load data

```{r input}
df <- read.csv("https://raw.githubusercontent.com/abbylmm/fake_job_posting/main/data/fake_job_postings.csv")
```

## Display n sample of the data

```{r}
df_fake_job <- df
sample_n(df_fake_job, 3)
```

## Summary data

```{r}
summary(df_fake_job)
```

## Check all the missing values - 'empty'

```{r}
skim_without_charts(df_fake_job)
```

## Split location to country, state, city and fill empty with NA

```{r}
df_fake_job[c("country", "state", "city")] <- str_split_fixed(df_fake_job$location, ", ", 3)
df_fake_job[c("country", "state", "city")][df_fake_job[c("country", "state", "city")] == ""] <- NA
```

## Split salary_range to min_salary, max_salary and fill empty with NA

```{r}
df_fake_job[c("min_salary", "max_salary")] <- str_split_fixed(df_fake_job$salary_range, "-", 2)
df_fake_job[c("min_salary", "max_salary")][df_fake_job[c("min_salary", "max_salary")] == ""] <- NA
```

## Drop location and salary_range

```{r}
df_fake_job <- select(df_fake_job, -c(location, salary_range))
```

## View the structure of data

```{r}
glimpse(df_fake_job)
class(df_fake_job)
```

## View column names

```{r}
names(df_fake_job)
```

## Check if any duplication id

```{r}
table(duplicated(df_fake_job$job_id))
# there is no duplication id
```

## Check for total missing values for each feature

```{r}
colSums(is.na(df_fake_job))
```

There are two missing values in 'benefits' column

## List rows with missing values

```{r}
missingdf <- df_fake_job[!complete.cases(df_fake_job), ]
sample_n(missingdf, 3)
```

## Visualize missing rates for each feature

```{r, warning=FALSE}
gg_miss_var(df_fake_job, show_pct = TRUE) + labs(y = "% Missing")
```

## Merge columns and create a new 'full_text' column

```{r}
viz_df <- select(df_fake_job, -c(max_salary, min_salary, state, city))
viz_df$full_text <- 
  paste(na.omit(viz_df$title), 
        na.omit(viz_df$country), 
        na.omit(viz_df$department), 
        na.omit(viz_df$company_profile), 
        na.omit(viz_df$description), 
        na.omit(viz_df$requirements), 
        na.omit(viz_df$benefits), 
        na.omit(viz_df$employment_type), 
        na.omit(viz_df$required_experience), 
        na.omit(viz_df$required_education), 
        na.omit(viz_df$industry), 
        na.omit(viz_df$function.))
viz_df[viz_df == ""] <- NA
# sample(viz_df, 3)
# write.csv(viz_df, "C:/Users/munmu/Documents/GitHub/fake_job_posting\\viz_df.csv", row.names = FALSE)
```

## Visualize missing profile for each feature

```{r, warning=FALSE}
plot_missing(viz_df)
```

## Heatplot of missingness across the dataframe

```{r, warning=FALSE}
vis_miss(viz_df)
```

## Drop columns

```{r}
model_df <- select(viz_df, 
                   -c(title, 
                      country, 
                      department, 
                      company_profile, 
                      description, 
                      requirements, 
                      benefits, 
                      employment_type, 
                      required_experience, 
                      required_education, 
                      industry, 
                      function.))
sample_n(model_df, 3)
# write.csv(model_df, "C:/Users/munmu/Documents/GitHub/fake_job_posting\\model_df.csv", row.names = FALSE)
```

## Check NA or missing values

```{r}
sum(is.na(model_df))
sum(model_df == "")
```

## Visualize missing values

```{r}
vis_miss(model_df)
vis_dat(model_df)
```

# Exploratory Data Analysis (EDA)

Before building our models, we performed exploratory data analysis to understand the dataset.

## Visualize fraud and real

```{r}
viz_df2 <- viz_df
viz_df2$fraudulent[viz_df2$fraudulent == 1] <- "Fraud"
viz_df2$fraudulent[viz_df2$fraudulent == 0] <- "Non Fraud"
count <- table(viz_df2$fraudulent)
bar <- barplot(count, 
               main="Proportion of fraudulent job postings", 
               xlab="fraudulent", 
               ylab="count", 
               col=c(rgb(0.3,0.1,0.4,0.6), rgb(0.3,0.9,0.4,0.6)))
text(bar, count/2, labels = count)
```

It is observable that there are 17,014 cases of legitimate job postings, while the number of fraudulent job postings is 866. The fraud rate of this dataset is 4.84%.

## Visualize country-wise job postings

```{r}
temp <- na.omit(subset(viz_df, select = c(country))) %>% 
  group_by(country) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>% 
  slice(1:10)

par(mar=c(6,4,4,4))
barplot(height=temp$n, 
        main="Top 10 country-wise job postings", 
        ylab="count", 
        col=brewer.pal(10, "Set3"), 
        names.arg=c("United States",
                    "United Kingdom",
                    "Greece",
                    "Canada",
                    "Germany",
                    "New Zealand",
                    "India",
                    "Australia",
                    "Philippines",
                    "Netherlands"), 
        cex.names=0.7, 
        las=2)
```

Top 10 countries with most of the number of job postings are US, GB, GR, CA, DE, NZ, IN, AU, PH, NL. United States listed 10,656 job postings, followed by 2,384 for United Kingdom and 940 for Greece.

## Visualize the industries

```{r}
temp <- na.omit(subset(viz_df, select = c(industry))) %>% 
  group_by(industry) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>% 
  slice(1:10)

par(mar=c(10,4,4,4))
barplot(height=temp$n, 
        names=temp$industry, 
        main="Top 10 industries", 
        ylab="count", 
        col=brewer.pal(10, "Set3"), 
        cex.names=0.6, 
        las=2)
```

Most job openings are IT related such as Information Technology and Services (1,734), Computer Software (1,376) and Internet (1,062).

## Visualize the departments

```{r}
temp <- na.omit(subset(viz_df, select = c(department))) %>% 
  group_by(department) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>% 
  slice(1:10)

par(mar=c(8,4,4,4))
barplot(height=temp$n, 
        names=temp$department, 
        main="Top 10 departments", 
        ylab="count", 
        col=brewer.pal(10, "Set3"), 
        cex.names=0.6, 
        las=2)
```

Top hiring departments are Sales (551), Engineering (487) and Marketing (401).

## Visualize the required experiences in the jobs

```{r}
viz_df %>% group_by(required_experience) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>% 
  slice(2:11) %>% 
  ggplot(aes(x=reorder(required_experience, -n), y = n)) + 
  geom_segment(aes(x=reorder(required_experience, -n), xend=reorder(required_experience, -n), y=0, yend=n), color="skyblue") + 
  geom_point(color="steelblue", size=2, alpha=1) + 
  theme_light() + 
  coord_flip() + 
  theme(panel.grid.major.y = element_blank(), 
        panel.border = element_blank(), 
        axis.ticks.y = element_blank()) + 
  theme_bw() + labs(title = "Listed jobs with required experiences", 
                    x = "Experience", 
                    y = "Count", 
                    fill = "Experience") + 
  geom_text(aes(label=round(n,0)), vjust=-0.6)
```

Mid-Senior level jobs are in demand, followed by entry level and associate.

## Visualize the required education in the jobs

```{r}
viz_df %>% group_by(required_education) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>% 
  slice(2:11) %>% 
  ggplot(aes(x=reorder(required_education, -n), y = n)) + 
  geom_segment(aes(x=reorder(required_education, -n), xend=reorder(required_education, -n), y=0, yend=n), color="skyblue") + 
  geom_point(color="steelblue", size=2, alpha=1) + 
  theme_light() + 
  coord_flip() + 
  theme(panel.grid.major.y = element_blank(), 
        panel.border = element_blank(), 
        axis.ticks.y = element_blank()) + 
  theme_bw() + labs(title = "Listed jobs with required education", 
                    x = "Education", 
                    y = "Count", 
                    fill = "Education") + 
  geom_text(aes(label=round(n,0)), vjust=-0.6)
```

Most of the education requirements in job ads are at least Bachelor's degree.

## Visualize fraudulent job postings based on employment types

```{r}
viz_df2 <- viz_df
viz_df2$employment_type <- ifelse(is.na(viz_df2$employment_type), "Missing", viz_df2$employment_type)
df1 <- subset(viz_df2, select = c(employment_type, fraudulent)) %>% 
  group_by(employment_type, fraudulent) %>% 
  summarize(yes = sum(fraudulent==1), .groups = 'drop') %>% 
  filter(fraudulent==1)
df2 <- subset(viz_df2, select = c(employment_type, fraudulent)) %>% 
  group_by(employment_type, fraudulent) %>% 
  summarize(no = sum(fraudulent==0), .groups = 'drop') %>% 
  filter(fraudulent==0)
df_new <- merge(df1, df2, by = c("employment_type")) %>% 
  group_by(employment_type) %>% 
  summarize(pct_fraud = round(yes/(yes+no), digits=3), 
            pct_non_fraud = 1-pct_fraud, .groups = 'drop') %>% 
  mutate(employment_type = factor(employment_type, 
                                  levels = c('Part-time',
                                             'Missing',
                                             'Other',
                                             'Full-time',
                                             'Contract',
                                             'Temporary')))
fig <- df_new %>% plot_ly(width = 700, height = 400)
fig <- fig %>% add_trace(x = ~employment_type, y = ~pct_non_fraud, type = 'bar', 
             text = ~paste0(pct_non_fraud*100,"%"), textposition = 'outside', name = 'pct_non_fraud', 
             marker = list(color = 'rgb(158,202,225)', 
                           line = list(color = 'rgb(8,48,107)', width = 0.8)))
fig <- fig %>% add_trace(x = ~employment_type, y = ~pct_fraud, type = 'bar', 
            text = ~paste0(pct_fraud*100,"%"), textposition = 'outside', name = 'pct_fraud', 
            marker = list(color = 'rgb(58,200,225)', 
                          line = list(color = 'rgb(8,48,107)', width = 0.8)))
fig <- fig %>% layout(title = "Employment types with % fraud and non-fraud",
         barmode = 'group',
         xaxis = list(title = "employment_type"),
         yaxis = list(title = "percentage"))
fig
```

The percentage of fraudulent job postings is the highest for part-time jobs, nearly 9%. Jobs without an employment type also have a high fraud rate, around 7%.

## Visualize fraudulent job postings based on required experiences

```{r}
viz_df2 <- viz_df
viz_df2$required_experience <- ifelse(is.na(viz_df2$required_experience), "Not Applicable", viz_df2$required_experience)
df1 <- subset(viz_df2, select = c(required_experience, fraudulent)) %>% 
  group_by(required_experience, fraudulent) %>% 
  summarize(yes = sum(fraudulent==1), .groups = 'drop') %>% 
  filter(fraudulent==1)
df2 <- subset(viz_df2, select = c(required_experience, fraudulent)) %>% 
  group_by(required_experience, fraudulent) %>% 
  summarize(no = sum(fraudulent==0), .groups = 'drop') %>% 
  filter(fraudulent==0)
df_new <- merge(df1, df2, by = c("required_experience")) %>% 
  group_by(required_experience) %>% 
  summarize(pct_fraud = round(yes/(yes+no), digits=3), 
            pct_non_fraud = 1-pct_fraud, .groups = 'drop') %>% 
  mutate(required_experience = factor(required_experience, 
                                      levels = c('Executive',
                                                 'Entry level',
                                                 'Not Applicable',
                                                 'Director',
                                                 'Mid-Senior level',
                                                 'Internship',
                                                 'Associate')))
fig <- df_new %>% plot_ly(width = 700, height = 400)
fig <- fig %>% add_trace(x = ~required_experience, y = ~pct_non_fraud, type = 'bar', 
             text = ~paste0(pct_non_fraud*100,"%"), textposition = 'outside', name = 'pct_non_fraud', 
             marker = list(color = 'rgb(158,202,225)', 
                           line = list(color = 'rgb(8,48,107)', width = 0.8)))
fig <- fig %>% add_trace(x = ~required_experience, y = ~pct_fraud, type = 'bar', 
            text = ~paste0(pct_fraud*100,"%"), textposition = 'outside', name = 'pct_fraud', 
            marker = list(color = 'rgb(58,200,225)', 
                          line = list(color = 'rgb(8,48,107)', width = 0.8)))
fig <- fig %>% layout(title = "Required experiences with % fraud and non-fraud",
         barmode = 'group',
         xaxis = list(title = "required_experience"),
         yaxis = list(title = "percentage"))
fig
```

Most executive or entry level jobs that require minimum qualifications and little experience have highest fraud rate, nearly 7%.

## Visualize fraudulent job postings based on job functions

```{r, warning=FALSE}
viz_df2 <- viz_df
viz_df2$fraudulent[viz_df2$fraudulent == 1] <- "Fraud"
viz_df2$fraudulent[viz_df2$fraudulent == 0] <- "Non Fraud"
temp <- na.omit(subset(viz_df2, select = c(function., fraudulent))) %>% 
  group_by(function., fraudulent) %>% 
  summarize(n = n(), .groups = 'drop') %>% 
  group_by(function.) %>% 
  summarize(pct_fraud = round(sum(n[fraudulent=="Fraud"]/sum(n)), digits=3), 
            pct_non_fraud = 1-pct_fraud, .groups = 'drop') %>% 
  arrange(desc(pct_fraud)) %>% 
  slice(1:10) %>% 
  mutate(function. = factor(function., 
                            levels = c('Administrative',
                                       'Financial Analyst',
                                       'Accounting/Auditing',
                                       'Distribution',
                                       'Other',
                                       'Finance',
                                       'Engineering',
                                       'Business Development',
                                       'Advertising',
                                       'Customer Service')))
melted_temp <- melt(temp, id = "function.")
ggplot(melted_temp, aes(x = function., y = value, fill = variable)) + 
  geom_bar(position = "fill", 
           stat = "identity", 
           color = "black", 
           width = 0.8) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.6)) + 
  scale_y_continuous(labels = scales::percent) + 
  geom_text(aes(label = paste0(value*100,"%")), 
            position = position_stack(vjust = 0.6), size = 2) + 
  ggtitle("Job functions with % fraud and non-fraud") + 
  xlab("function") + 
  ylab("percentage")
```

The function with highest fraudulent job postings is Administrative, close to 19%, followed by Financial Analyst, Accounting/Auditing. Admin jobs seem most suspicious. Possibly, it's easy for scammers to disguise their scams.

## Visualize fraudulent job postings based on required education

```{r, warning=FALSE}
temp <- na.omit(subset(viz_df2, select = c(required_education, fraudulent))) %>% 
  group_by(required_education, fraudulent) %>% 
  summarize(n = n(), .groups = 'drop') %>% 
  group_by(required_education) %>% 
  summarize(pct_fraud = round(sum(n[fraudulent=="Fraud"]/sum(n)), digits=3), 
            pct_non_fraud = 1-pct_fraud, .groups = 'drop') %>% 
  arrange(desc(pct_fraud)) %>% 
  slice(1:10) %>% 
  mutate(required_education = factor(required_education, 
                                     levels = c("Some High School Coursework",
                                                "Certification",
                                                "High School or equivalent",
                                                "Master's Degree",
                                                "Professional",
                                                "Unspecified",
                                                "Doctorate",
                                                "Some College Coursework Completed",
                                                "Associate Degree",
                                                "Bachelor's Degree")))
melted_temp <- melt(temp, id = "required_education")
ggplot(melted_temp, aes(x = required_education, y = value, fill = variable)) + 
  geom_bar(position = "fill", 
           stat = "identity", 
           color = "black", 
           width = 0.8) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.6)) + 
  scale_y_continuous(labels = scales::percent) + 
  geom_text(aes(label = paste0(value*100,"%")), 
            position = position_stack(vjust = 0.6), size = 2) + 
  ggtitle("Required education with % fraud and non-fraud") + 
  xlab("required_education") + 
  ylab("percentage")
```

As high as 74% of fake jobs require little educational credentials - "Some High School Coursework".

## Word Cloud

To visualize the fraud and real job postings, the WordCloud is used to see the top occurring keywords in the data. To do so, fraud and real job postings are separated into two text files and WordCloud has plotted accordingly.

## Word Cloud of fraudulent job postings

```{r, warning=FALSE}
selected_df <- subset(viz_df, fraudulent == 1)

# Create a vector containing only the text
text <- selected_df$title

# Create a corpus
docs <- Corpus(VectorSource(text))

docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))

dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix), decreasing=TRUE)
df <- data.frame(word = names(words), freq=words)

wordcloud(words = df$word, freq = df$freq, min.freq = 1, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
```

Many of the fraudulent job postings have common keywords in the job titles - “Data Entry”, "Administrative", “Home Based”, “Earn Daily".

## Word Cloud of NON-fraudulent job postings

```{r, warning=FALSE}
selected_df <- subset(viz_df, fraudulent == 0)

# Create a vector containing only the text
text <- selected_df$title

# Create a corpus
docs <- Corpus(VectorSource(text))

docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))

dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix), decreasing=TRUE)
df <- data.frame(word = names(words), freq=words)

wordcloud(words = df$word, freq = df$freq, min.freq = 1, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
```

Many of the NON-fraudulent job postings have common keywords in the job titles - "Manager", "Developer", "Engineer".

# Modeling

Before modeling, a final dataset is determined. This project will use a dataset with these features for the final analysis:

-   fraudulent (target variable)
-   telecommuting
-   has_company_logo
-   has_questions
-   full_text: a combination of title, country, department, company_profile, description, requirements, benefits, employment_type, required_experience, required_education, industry and function

Three supervised machine learning algorithms used in the project are:

-   Logistic Regression
-   Random Forest
-   K-Nearest Neighbor (KNN)

## Data pre-process (full_text)

For this analysis, the entire full_text column is converted to a DocumentTermMatrix and then convert to a dataframe.

```{r, warning=FALSE}
# temp <- subset(model_df, fraudulent == 1)
docs <- Corpus(VectorSource(model_df$full_text))
docs <- docs %>%
  tm_map(removeNumbers) %>% # Remove numbers
  tm_map(removePunctuation) %>% # Remove punctuation
  tm_map(stripWhitespace) # Eliminate extra white spaces
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))

# Convert each full_text into a row with columns containing each term in the document and giving the frequency of unique words used in the full_text
dtm <- DocumentTermMatrix(docs)
sparse_data <- removeSparseTerms(dtm, 0.90) # Remove sparse data
```

```{r, warning=FALSE}
# Convert to dataframe for further analysis
sparse_data_df <- as.data.frame(as.matrix(sparse_data))
final_df <- subset(sparse_data_df, select = -c(`–`))

# Add other variables
final_df$telecommuting <- model_df$telecommuting
final_df$has_company_logo <- model_df$has_company_logo
final_df$has_questions <- model_df$has_questions
final_df$fraudulent <- model_df$fraudulent
```

## View the dimension of the dataframe

```{r}
dim(final_df)
# 17880 rows, 313 columns
```

## Visualize data

```{r}
# Histogram
par(mfrow=c(2,2))
for(i in 310:313) {
    hist(final_df[,i], main=names(final_df)[i], border="blue", col="yellow")
}
```

```{r}
# Boxplot
par(mfrow=c(2,2))
for(i in 310:313) {
    boxplot(final_df[,i], main=names(final_df)[i], border="blue", col="yellow")
}
```

## Correlation

A correlation matrix is created to visualize the numeric data relationship.

```{r}
# Calculate the correlation between each pair of numeric variables
selected_df<-final_df[, 310:313]
corr_df <- round(cor(selected_df), 2)
corr_df
```

Visualise correlation heatmap

```{r}

# reduce the size of correlation matrix
melted_corr_mat<-melt(corr_df)

# plotting the correlation heatmap

ggplot(data = melted_corr_mat, aes(x = Var1, y = Var2, fill = value))+
  geom_tile() +
  geom_text(aes( label = value), color = "black", size = 4)
```

It can be seen that all features are not highly correlated however has_company_logo and has_questions have negative correlation with fraudulent. This indicates that If the job posting has a company logo or with questions, the likelihood of fraudulent decreases.

## Split data into 70% training, 30% testing

```{r}
# Using the same seed value, reproduce the division of the training and testing sets
set.seed(123)
train_index <- sample(dim(final_df)[1], 0.7 * dim(final_df)[1])
model_dftrain<- final_df[train_index, ]
model_dftest <- final_df[-train_index, ]
paste("train sample size: ", dim(model_dftrain)[1])
paste("test sample size: ", dim(model_dftest)[1])
```

## View training set

```{r}
sample_n(model_dftrain, 3)
```

## Convert the dependent variable as a factor

```{r}
model_dftrain$fraudulent = as.factor(model_dftrain$fraudulent)
model_dftest$fraudulent = as.factor(model_dftest$fraudulent)
```

## Logistic Regression

```{r, warning=FALSE}
# Train logistic regression
lr_model <- glm(formula = fraudulent ~ ., family = "binomial", data = model_dftrain)
```

### Predict the testing set

```{r}
lr_pred_test <- predict(lr_model, newdata = model_dftest, type = "response")
```

```{r}
test <- model_dftest
glm.probs = predict(lr_model, newdata = test, type = "response")
test$pred_glm = ifelse(glm.probs > 0.5, "1", "0")
test$pred_glm = as.factor(test$pred_glm)
```

### Calculate AUC of the model

```{r}
calcAUC <- function(predcol, outcol) {
  perf <- performance(prediction(as.numeric(predcol), outcol == 1), "auc")
  as.numeric(perf@y.values)
}

paste("AUC of Logistic Regression is", round(calcAUC(lr_pred_test, model_dftest$fraudulent), digits=4))
```

## Random Forest

```{r}
# Train random forest
trcontrol <- trainControl(method = "repeatedcv", number = 2, repeats = 1, search = "random", verboseIter = TRUE)
grid <- data.frame(mtry = c(100))
rf_model <- train(fraudulent ~ ., method = "rf", data = model_dftrain, ntree = 200, trControl = trcontrol, tuneGrid = grid)
rf_model
```

### Predict the testing set

```{r}
rf_pred_test <- predict(rf_model, newdata = model_dftest)
```

### Calculate AUC of the model

```{r}
paste("AUC of Random Forest is", round(calcAUC(rf_pred_test, model_dftest$fraudulent), digits=4))
```

## K-Nearest Neighbor (KNN)

```{r}
# Train knn
knn <- kknn(fraudulent ~ ., model_dftrain, model_dftest, k = 25)
# View(knn)
```

### Predict the testing set

```{r}
knn_pred_test <- predict(knn, newdata = model_dftest)
```

### Calculate AUC of the model

```{r}
paste("AUC of KNN is", round(calcAUC(knn_pred_test, model_dftest$fraudulent), digits=4))
```

# Evaluation

Accuracy and area under the curve (AUC) are used to evaluate the effectiveness of models in terms of classifying real and fake job postings. However, the dataset used for training is highly imbalanced. Thus, it is necessary to use F1 scores, precision, and recall to evaluate the model's ability to identify both real and fake job postings.

-   Accuracy score: Metric that provides a general idea of the model performance.
-   AUC score: Measure how well the model can distinguish real and fake job postings.
-   Precision score: Percentage of positive predictions are accurate.
-   Recall score: Percentage of positive results that have been classified correctly by the model.
-   F1 score: Harmonic mean of precision and recall.

```{r}
# Error Metrics -- Confusion Matrix
err_metric=function(CM)
{
  TN = CM[1,1]
  TP = CM[2,2]
  FP = CM[1,2]
  FN = CM[2,1]
  precision = (TP)/(TP+FP)
  recall_score = (FP)/(FP+TN)
  
  f1_score = 2*((precision*recall_score)/(precision+recall_score))
  accuracy_model = (TP+TN)/(TP+TN+FP+FN)
  False_positive_rate = (FP)/(FP+TN)
  False_negative_rate = (FN)/(FN+TP)
  
  print(paste("Precision value of the model: ", round(precision,2)))
  print(paste("Accuracy of the model: ", round(accuracy_model,2)))
  print(paste("Recall value of the model: ", round(recall_score,2)))
  print(paste("False Positive rate of the model: ", round(False_positive_rate,2)))
  print(paste("False Negative rate of the model: ", round(False_negative_rate,2)))
  print(paste("f1 score of the model: ", round(f1_score,2)))
}
```

## Confusion Matrix and Error Metrics of Logistic Regression

```{r}
confMatrix_lr = table(test$pred_glm, test$fraudulent)
print(confMatrix_lr)
err_metric(confMatrix_lr)
```

## Confusion Matrix and Error Metrics of Random Forest

```{r}
confMatrix_rf = table(rf_pred_test, model_dftest$fraudulent)
print(confMatrix_rf)
err_metric(confMatrix_rf)
```

## Confusion Matrix and Error Metrics of KNN

```{r}
confMatrix_knn = table(knn_pred_test, model_dftest$fraudulent)
print(confMatrix_knn)
err_metric(confMatrix_knn)
```

## Summary of Results

| Metric    | Logistic Regression | Random Forest | KNN  |
|-----------|---------------------|---------------|------|
| Accuracy  | 0.97                | 0.98          | 0.97 |
| Precision | 0.58                | 0.61          | 0.54 |
| Recall    | 0.02                | 0.02          | 0.02 |
| F1        | 0.04                | 0.04          | 0.05 |
| AUC       | 0.95                | 0.80          | 0.77 |

The Random Forest has achieved the best accuracy and precision while its f1 score is a little bit lower than KNN. However, Logistic Regression has achieved the highest AUC than others while its precision is lower than Random Forest. Given the precision score, we can conclude that Random Forest is the best in terms of classifying real and fake job postings.

# Result Analysis Summary

1.  What are the key features/characteristics of fraudulent job postings?

Based on the correlation analysis, all of the features are not highly correlated to our target feature (fraudulent) and therefore, it is difficult to find out the key features or characteristics of fraudulent job postings. However, it can be seen that has_company_logo and has_questions features have negative correlation with fraudulent. This may indicates that If the job posting has a company logo or with questions, the likelihood of fraudulent decreases.

1.  Which classification model is the best to determine whether the job is real or not?

Random Forest is the best classification model to determine whether the job is real or not. This conclusion was made in regard to Random Forest model has shown the best accuracy and precision result compared to other models.

1.  Other findings

-   74% of fake jobs require little educational credentials - "Some High School Coursework". This may indicates the target of fake job postings is jobseeker with little educational credentials such as highschooler.
-   Most executive or entry level jobs that require minimum qualifications and little experience have highest fraud rate, nearly 7%. This information implies that job seekers with lack of experience such as fresh graduates is most likely the target of this fake job postings.
-   Many of the fraudulent job postings have common keywords in the job titles - “Data Entry”, "Administrative", “Home Based”, “Earn Daily". These are the words that can attract the attention of the jobseekers.

# Limitation and Improvement

Since the dataset is highly imbalanced where most of the job postings are legitimate, and only few are fraudulent. Thus, real jobs are being identified quite well. Techniques to handle imbalanced data like SMOTE can be applied to make a fair comparison between real and fraudulent jobs. Besides, other NLP processing like TF-IDF vectorizer can be chosen to discover the best possible numerical/vectorial representation of the text strings for running ML models.

# Conclusion

In most instances, if something appears too good to be true, it probably is. Most of the fraudulent job description and requirements are vague and too good to be true such as easy work for unrealistic pay. Be aware of part-time, entry-level jobs that require minimum qualifications and little experience like data entry and administrative. Home based and job listings without company logo can be alarming. In terms of classification models, Random Forest gives the best accuracy and precision, however better results can be achieved with a more balanced dataset with sufficient use cases for both real and fake job postings. Finally, with a little research, we can not only find out if a company and a job are legit, but also discover if the company is a right fit.
